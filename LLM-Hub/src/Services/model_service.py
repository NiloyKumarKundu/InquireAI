# from langchain_ollama.chat_models import ChatOllama
# import logging

# def get_model():
#     try:
#         llm = ChatOllama(
#             model="llama3.2",
#             convert_system_message_to_human=True
#         )
#         return llm
#     except Exception as e:
#         logging.error(f"Failed to load model: {e}")
#         raise
